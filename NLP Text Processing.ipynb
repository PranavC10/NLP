{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b157ca26",
   "metadata": {},
   "source": [
    "# NLP "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d20cda9",
   "metadata": {},
   "source": [
    "Natural language processing is a subset of Artificial intelligence that helps computers to understand, interpret, and utilize the human languages. \n",
    "\n",
    "NLP allows computers to communicate with peoples using human languages. \n",
    "\n",
    "NLP also provides computers with the ability to read text, hear speech, and try to interpret it. NLP draws several disciplines, including Computational linguistics and computer science, as this attempts to fill the gap in between human and computer communication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb13dc3",
   "metadata": {},
   "source": [
    "Letâ€™s take a look at 11 of the most interesting applications of natural language processing  in business:\n",
    "\n",
    "Sentiment Analysis\n",
    "\n",
    "Text Classification\n",
    "\n",
    "Chatbots & Virtual Assistants\n",
    "\n",
    "Text Extraction\n",
    "\n",
    "Machine Translation\n",
    "\n",
    "Text Summarization\n",
    "\n",
    "\n",
    "Auto-Correct\n",
    "\n",
    "Intent Classification\n",
    "\n",
    "Urgency Detection\n",
    "\n",
    "Speech Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b14f28",
   "metadata": {},
   "source": [
    "## Text Processing \n",
    "\n",
    "Lets consider we have textual data with lots of variation.we need to apply many of pre-processing steps to the data to transform those words into numerical features that work with machine learning algorithms.In the end ML alogorithms understand numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3625b41",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4ac7d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\pranav\\anaconda3\\lib\\site-packages (3.6.5)\n",
      "Requirement already satisfied: click in c:\\users\\pranav\\anaconda3\\lib\\site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\pranav\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\pranav\\anaconda3\\lib\\site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pranav\\anaconda3\\lib\\site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\pranav\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk # in case if its not installed "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf980fe",
   "metadata": {},
   "source": [
    "The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing for English written in the Python programming language.\n",
    "NLTK is a Python package that you can use for NLP. A lot of the data that you could be analyzing is unstructured data and contains human-readable text. NLTK is pandas of textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2f95f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries \n",
    "import nltk # Natural Language Toolkit\n",
    "import string # Common string operations\n",
    "import re #This module provides regular expression matching operations  # this already covered in python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c061df6d",
   "metadata": {},
   "source": [
    "### Basic Text Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a563c495",
   "metadata": {},
   "source": [
    "### Upercase & Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a1de388",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1= \"I am Going to Learn NLP\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27d6a646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am Going to Learn NLP\n"
     ]
    }
   ],
   "source": [
    "print(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c97f8a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am going to learn nlp'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.lower()  # Text lowercase We do lowercase the text to reduce the size of the vocabulary of our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79f2093a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I AM GOING TO LEARN NLP'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85de350b",
   "metadata": {},
   "source": [
    "### Remove Punctuation # common sense / BI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18feb2ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Guys are you excited  we will start CV soon '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rem_punct(text): \n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    return text.translate(translator) \n",
    "  \n",
    "input_str = \"Hi!, Guys are you excited ?? we will start CV soon !!\"\n",
    "rem_punct(input_str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ccbfb8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e8af25",
   "metadata": {},
   "source": [
    "### Remove default stopwords:\n",
    "\n",
    "Stopwords are words that do not contribute to the meaning of the sentence. Hence, they can be safely removed without causing any change in the meaning of a sentence. The NLTK(Natural Language Toolkit) library has the set of stopwords and we can use these to remove stopwords from our text and return a list of word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4cf8433a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Pranav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Pranav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords #A corpus is a collection of authentic text or audio organized into datasets. Authentic here means text written or audio spoken by a native of the language or dialect. A corpus can be made up of everything from newspapers, novels, recipes, radio broadcasts to television shows, movies, and tweets.\n",
    "from nltk.tokenize import word_tokenize \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3725c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text): \n",
    "    stop_words = set(stopwords.words(\"english\")) \n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words] #python code to do tokenization excluding stop words \n",
    "    return filtered_text \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2519f98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'subset',\n",
       " 'Artificial',\n",
       " 'intelligence',\n",
       " 'helps',\n",
       " 'computers',\n",
       " 'understand',\n",
       " 'human',\n",
       " 'languages']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_text = \"Natural language processing is a subset of Artificial intelligence that helps computers to understand the human languages\"\n",
    "remove_stopwords(ex_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa1db0d",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "From Stemming we will process of getting the root form of a word. Root or Stem is the part to which inflextional affixes(like -ed, -ize, etc) are added. We would create the stem words by removing the prefix of suffix of a word. So, stemming a word may not result in actual words.\n",
    "\n",
    "For Example: Mangoes ---> Mango\n",
    "\n",
    "             Boys ---> Boy\n",
    "             \n",
    "             going ---> go\n",
    "             \n",
    "             \n",
    "If our sentences are not in tokens, then we need to convert it into tokens. After we converted strings of text into tokens, then we can convert those word tokens into their root form. These are the Porter stemmer, the snowball stemmer, and the Lancaster Stemmer. We usually use Porter stemmer among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3db26568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'is',\n",
       " 'the',\n",
       " 'new',\n",
       " 'revolut',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " ',',\n",
       " 'in',\n",
       " 'a',\n",
       " 'day',\n",
       " 'one',\n",
       " 'individu',\n",
       " 'would',\n",
       " 'gener',\n",
       " 'terabyt',\n",
       " 'of',\n",
       " 'data',\n",
       " '.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing nltk's porter stemmer \n",
    "from nltk.stem.porter import PorterStemmer #Stemming\n",
    "from nltk.tokenize import word_tokenize \n",
    "stem1 = PorterStemmer() \n",
    "  \n",
    "# stem words in the list of tokenised words \n",
    "def s_words(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    stems = [stem1.stem(word) for word in word_tokens] \n",
    "    return stems \n",
    "  \n",
    "text = 'Data is the new revolution in the World, in a day one individual would generate terabytes of data.'\n",
    "s_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3759aaf5",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "As stemming, lemmatization do the same but the only difference is that lemmatization ensures that root word belongs to the language. Because of the use of lemmatization we will get the valid words. In NLTK(Natural language Toolkit), we use WordLemmatizer to get the lemmas of words. We also need to provide a context for the lemmatization.So, we added pos(parts-of-speech) as a parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac1c1c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Pranav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Data',\n",
       " 'be',\n",
       " 'the',\n",
       " 'new',\n",
       " 'revolution',\n",
       " 'in',\n",
       " 'the',\n",
       " 'World',\n",
       " ',',\n",
       " 'in',\n",
       " 'a',\n",
       " 'day',\n",
       " 'one',\n",
       " 'individual',\n",
       " 'would',\n",
       " 'generate',\n",
       " 'terabytes',\n",
       " 'of',\n",
       " 'data',\n",
       " '.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import wordnet \n",
    "from nltk.tokenize import word_tokenize \n",
    "lemma = wordnet.WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "# lemmatize string \n",
    "def lemmatize_word(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    # provide context i.e. part-of-speech(pos)\n",
    "    lemmas = [lemma.lemmatize(word, pos ='v') for word in word_tokens] \n",
    "    return lemmas \n",
    "  \n",
    "text = 'Data is the new revolution in the World, in a day one individual would generate terabytes of data.'\n",
    "lemmatize_word(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceba5f7a",
   "metadata": {},
   "source": [
    "### Parts of Speech (POS) Tagging\n",
    "\n",
    "The pos(parts of speech) explain you how a word is used in a sentence. In the sentence, a word have different contexts and semantic meanings. The basic natural language processing(NLP) models like bag-of-words(bow) fails to identify these relation between the words. For that we use pos tagging to mark a word to its pos tag based on its context in the data. Pos is also used to extract rlationship between the words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d19868f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Pranav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Are', 'NNP'),\n",
       " ('you', 'PRP'),\n",
       " ('going', 'VBG'),\n",
       " ('of', 'IN'),\n",
       " ('somewhere', 'RB'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing tokenize library\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk import pos_tag \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "  \n",
    "# convert text into word_tokens with their tags \n",
    "def pos_tagg(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    return pos_tag(word_tokens) \n",
    "  \n",
    "pos_tagg('Are you going of somewhere?') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26909551",
   "metadata": {},
   "source": [
    "In the above example NNP stands for Proper noun, PRP stands for personal noun, IN as Preposition. We can get all the details pos tags using the Penn Treebank tagset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f54537e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93024a74",
   "metadata": {},
   "source": [
    "# RE to extract email from textual data \n",
    "1. Create text file in that add some email and other text\n",
    "2. Read text file into python \n",
    "3. Extract all emails in the text file\n",
    "4. Store it in csv/excel format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeb02b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I will', 'will go', 'go to', 'to United', 'United States']\n"
     ]
    }
   ],
   "source": [
    "Sent = \"I will go to United States\"\n",
    "lst_sent = Sent.split (\" \")\n",
    "of_bigrams_in = []\n",
    "for i in range(len(lst_sent)- 1):\n",
    "   of_bigrams_in.append(lst_sent[i]+ \" \" + lst_sent[ i + 1])\n",
    "   \n",
    "    \n",
    "print(of_bigrams_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afa5e68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "punctuation_pattern = re.compile(r\"\" \"[.,!? \"\"] \"\" \" )\n",
    "\n",
    "sent = \"I will go to United States\"\n",
    "no_punctuation_sent = re.sub(punctuation_pattern , \" \" , sent )\n",
    "lst_sent = no_punctuation_sent.split (\" \")\n",
    "trigram = []\n",
    "for i in range(len(lst_sent)- 2):\n",
    "   trigram.append(lst_sent[i] + \" \" + lst_sent[i + 1] + \" \" +lst_sent[i + 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "650d672d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I will go', 'will go to', 'go to United', 'to United States']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01254b44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
